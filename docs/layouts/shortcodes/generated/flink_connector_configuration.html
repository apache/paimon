{{/*
Licensed to the Apache Software Foundation (ASF) under one
or more contributor license agreements.  See the NOTICE file
distributed with this work for additional information
regarding copyright ownership.  The ASF licenses this file
to you under the Apache License, Version 2.0 (the
"License"); you may not use this file except in compliance
with the License.  You may obtain a copy of the License at

  http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing,
software distributed under the License is distributed on an
"AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
KIND, either express or implied.  See the License for the
specific language governing permissions and limitations
under the License.
*/}}
<table class="configuration table table-bordered">
    <thead>
        <tr>
            <th class="text-left" style="width: 20%">Key</th>
            <th class="text-left" style="width: 15%">Default</th>
            <th class="text-left" style="width: 10%">Type</th>
            <th class="text-left" style="width: 55%">Description</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td><h5>changelog.precommit-compact.thread-num</h5></td>
            <td style="word-wrap: break-word;">(none)</td>
            <td>Integer</td>
            <td>Maximum number of threads to copy bytes from small changelog files. By default is the number of processors available to the Java virtual machine.</td>
        </tr>
        <tr>
            <td><h5>commit.custom-listeners</h5></td>
            <td style="word-wrap: break-word;">(none)</td>
            <td>String</td>
            <td>Commit listener will be called after a successful commit. This option list custom commit listener identifiers separated by comma.</td>
        </tr>
        <tr>
            <td><h5>end-input.watermark</h5></td>
            <td style="word-wrap: break-word;">(none)</td>
            <td>Long</td>
            <td>Optional endInput watermark used in case of batch mode or bounded stream.</td>
        </tr>
        <tr>
            <td><h5>filesystem.job-level-settings.enabled</h5></td>
            <td style="word-wrap: break-word;">true</td>
            <td>Boolean</td>
            <td>Enable pass job level filesystem settings to table file IO.</td>
        </tr>
        <tr>
            <td><h5>lookup.async</h5></td>
            <td style="word-wrap: break-word;">false</td>
            <td>Boolean</td>
            <td>Whether to enable async lookup join.</td>
        </tr>
        <tr>
            <td><h5>lookup.async-thread-number</h5></td>
            <td style="word-wrap: break-word;">16</td>
            <td>Integer</td>
            <td>The thread number for lookup async.</td>
        </tr>
        <tr>
            <td><h5>lookup.bootstrap-parallelism</h5></td>
            <td style="word-wrap: break-word;">4</td>
            <td>Integer</td>
            <td>The parallelism for bootstrap in a single task for lookup join.</td>
        </tr>
        <tr>
            <td><h5>lookup.cache</h5></td>
            <td style="word-wrap: break-word;">AUTO</td>
            <td><p>Enum</p></td>
            <td>The cache mode of lookup join.<br /><br />Possible values:<ul><li>"AUTO"</li><li>"FULL"</li><li>"MEMORY"</li></ul></td>
        </tr>
        <tr>
            <td><h5>lookup.dynamic-partition.refresh-interval</h5></td>
            <td style="word-wrap: break-word;">1 h</td>
            <td>Duration</td>
            <td>Specific dynamic partition refresh interval for lookup, scan all partitions and obtain corresponding partition.</td>
        </tr>
        <tr>
            <td><h5>lookup.refresh.async</h5></td>
            <td style="word-wrap: break-word;">false</td>
            <td>Boolean</td>
            <td>Whether to refresh lookup table in an async thread.</td>
        </tr>
        <tr>
            <td><h5>lookup.refresh.async.pending-snapshot-count</h5></td>
            <td style="word-wrap: break-word;">5</td>
            <td>Integer</td>
            <td>If the pending snapshot count exceeds the threshold, lookup operator will refresh the table in sync.</td>
        </tr>
        <tr>
            <td><h5>lookup.refresh.time-periods-blacklist</h5></td>
            <td style="word-wrap: break-word;">(none)</td>
            <td>String</td>
            <td>The blacklist contains several time periods. During these time periods, the lookup table's cache refreshing is forbidden. Blacklist format is start1-&gt;end1,start2-&gt;end2,... , and the time format is yyyy-MM-dd HH:mm. Only used when lookup table is FULL cache mode.</td>
        </tr>
        <tr>
            <td><h5>partition.idle-time-to-done</h5></td>
            <td style="word-wrap: break-word;">(none)</td>
            <td>Duration</td>
            <td>Set a time duration when a partition has no new data after this time duration, mark the done status to indicate that the data is ready.</td>
        </tr>
        <tr>
            <td><h5>partition.mark-done-action.mode</h5></td>
            <td style="word-wrap: break-word;">process-time</td>
            <td><p>Enum</p></td>
            <td>How to trigger partition mark done action.<br /><br />Possible values:<ul><li>"process-time": Based on the time of the machine, mark the partition done once the processing time passes period time plus delay.</li><li>"watermark": Based on the watermark of the input, mark the partition done once the watermark passes period time plus delay.</li></ul></td>
        </tr>
        <tr>
            <td><h5>partition.mark-done.recover-from-state</h5></td>
            <td style="word-wrap: break-word;">true</td>
            <td>Boolean</td>
            <td>Whether trigger partition mark done when recover from state.</td>
        </tr>
        <tr>
            <td><h5>partition.time-interval</h5></td>
            <td style="word-wrap: break-word;">(none)</td>
            <td>Duration</td>
            <td>You can specify time interval for partition, for example, daily partition is '1 d', hourly partition is '1 h'.</td>
        </tr>
        <tr>
            <td><h5>postpone.default-bucket-num</h5></td>
            <td style="word-wrap: break-word;">1</td>
            <td>Integer</td>
            <td>Bucket number for the partitions compacted for the first time in postpone bucket tables.</td>
        </tr>
        <tr>
            <td><h5>precommit-compact</h5></td>
            <td style="word-wrap: break-word;">false</td>
            <td>Boolean</td>
            <td>If true, it will add a compact coordinator and worker operator after the writer operator,in order to compact several changelog files (for primary key tables) or newly created data files (for unaware bucket tables) from the same partition into large ones, which can decrease the number of small files.</td>
        </tr>
        <tr>
            <td><h5>read.shuffle-bucket-with-partition</h5></td>
            <td style="word-wrap: break-word;">true</td>
            <td>Boolean</td>
            <td>Whether shuffle by partition and bucket when read.</td>
        </tr>
        <tr>
            <td><h5>scan.bounded</h5></td>
            <td style="word-wrap: break-word;">(none)</td>
            <td>Boolean</td>
            <td>Bounded mode for Paimon consumer. By default, Paimon automatically selects bounded mode based on the mode of the Flink job.</td>
        </tr>
        <tr>
            <td><h5>scan.dedicated-split-generation</h5></td>
            <td style="word-wrap: break-word;">false</td>
            <td>Boolean</td>
            <td>If true, the split generation process would be performed during runtime on a Flink task, instead of on the JobManager during initialization phase.</td>
        </tr>
        <tr>
            <td><h5>scan.infer-parallelism</h5></td>
            <td style="word-wrap: break-word;">true</td>
            <td>Boolean</td>
            <td>If it is false, parallelism of source are set by global parallelism. Otherwise, source parallelism is inferred from splits number (batch mode) or bucket number(streaming mode).</td>
        </tr>
        <tr>
            <td><h5>scan.infer-parallelism.max</h5></td>
            <td style="word-wrap: break-word;">1024</td>
            <td>Integer</td>
            <td>If scan.infer-parallelism is true, limit the parallelism of source through this option.</td>
        </tr>
        <tr>
            <td><h5>scan.max-snapshot.count</h5></td>
            <td style="word-wrap: break-word;">-1</td>
            <td>Integer</td>
            <td>The max snapshot count to scan per checkpoint. Not limited when it's negative.</td>
        </tr>
        <tr>
            <td><h5>scan.parallelism</h5></td>
            <td style="word-wrap: break-word;">(none)</td>
            <td>Integer</td>
            <td>Define a custom parallelism for the scan source. By default, if this option is not defined, the planner will derive the parallelism for each statement individually by also considering the global configuration. If user enable the scan.infer-parallelism, the planner will derive the parallelism by inferred parallelism.</td>
        </tr>
        <tr>
            <td><h5>scan.partitions</h5></td>
            <td style="word-wrap: break-word;">(none)</td>
            <td>String</td>
            <td>Specify the partitions to scan. Partitions should be given in the form of key1=value1,key2=value2. Partition keys not specified will be filled with the value of partition.default-name. Multiple partitions should be separated by semicolon (;). This option can support normal source tables and lookup join tables. There are two special values max_pt() and max_two_pt() are also supported to specify the (two) partition(s) with the largest partition value. For lookup source, the max partition(s) will be periodically refreshed; for normal source, the max partition(s) will be determined before job running without refreshing even for streaming jobs.</td>
        </tr>
        <tr>
            <td><h5>scan.remove-normalize</h5></td>
            <td style="word-wrap: break-word;">false</td>
            <td>Boolean</td>
            <td>Whether to force the removal of the normalize node when streaming read. Note: This is dangerous and is likely to cause data errors if downstream is used to calculate aggregation and the input is not complete changelog.</td>
        </tr>
        <tr>
            <td><h5>scan.split-enumerator.batch-size</h5></td>
            <td style="word-wrap: break-word;">10</td>
            <td>Integer</td>
            <td>How many splits should assign to subtask per batch in StaticFileStoreSplitEnumerator to avoid exceed `akka.framesize` limit.</td>
        </tr>
        <tr>
            <td><h5>scan.split-enumerator.mode</h5></td>
            <td style="word-wrap: break-word;">fair</td>
            <td><p>Enum</p></td>
            <td>The mode used by StaticFileStoreSplitEnumerator to assign splits.<br /><br />Possible values:<ul><li>"fair": Distribute splits evenly when batch reading to prevent a few tasks from reading all.</li><li>"preemptive": Distribute splits preemptively according to the consumption speed of the task.</li></ul></td>
        </tr>
        <tr>
            <td><h5>scan.watermark.alignment.group</h5></td>
            <td style="word-wrap: break-word;">(none)</td>
            <td>String</td>
            <td>A group of sources to align watermarks.</td>
        </tr>
        <tr>
            <td><h5>scan.watermark.alignment.max-drift</h5></td>
            <td style="word-wrap: break-word;">(none)</td>
            <td>Duration</td>
            <td>Maximal drift to align watermarks, before we pause consuming from the source/task/partition.</td>
        </tr>
        <tr>
            <td><h5>scan.watermark.alignment.update-interval</h5></td>
            <td style="word-wrap: break-word;">1 s</td>
            <td>Duration</td>
            <td>How often tasks should notify coordinator about the current watermark and how often the coordinator should announce the maximal aligned watermark.</td>
        </tr>
        <tr>
            <td><h5>scan.watermark.emit.strategy</h5></td>
            <td style="word-wrap: break-word;">on-event</td>
            <td><p>Enum</p></td>
            <td>Emit strategy for watermark generation.<br /><br />Possible values:<ul><li>"on-periodic": Emit watermark periodically, interval is controlled by Flink 'pipeline.auto-watermark-interval'.</li><li>"on-event": Emit watermark per record.</li></ul></td>
        </tr>
        <tr>
            <td><h5>scan.watermark.idle-timeout</h5></td>
            <td style="word-wrap: break-word;">(none)</td>
            <td>Duration</td>
            <td>If no records flow in a partition of a stream for that amount of time, then that partition is considered "idle" and will not hold back the progress of watermarks in downstream operators.</td>
        </tr>
        <tr>
            <td><h5>sink.clustering.sample-factor</h5></td>
            <td style="word-wrap: break-word;">100</td>
            <td>Integer</td>
            <td>Specifies the sample factor. Let S represent the total number of samples, F represent the sample factor, and P represent the sink parallelism, then S=FÃ—P. The minimum allowed sample factor is 20.</td>
        </tr>
        <tr>
            <td><h5>sink.clustering.sort-in-cluster</h5></td>
            <td style="word-wrap: break-word;">true</td>
            <td>Boolean</td>
            <td>Indicates whether to further sort data belonged to each sink task after range partitioning.</td>
        </tr>
        <tr>
            <td><h5>sink.committer-cpu</h5></td>
            <td style="word-wrap: break-word;">1.0</td>
            <td>Double</td>
            <td>Sink committer cpu to control cpu cores of global committer.</td>
        </tr>
        <tr>
            <td><h5>sink.committer-memory</h5></td>
            <td style="word-wrap: break-word;">(none)</td>
            <td>MemorySize</td>
            <td>Sink committer memory to control heap memory of global committer.</td>
        </tr>
        <tr>
            <td><h5>sink.committer-operator-chaining</h5></td>
            <td style="word-wrap: break-word;">true</td>
            <td>Boolean</td>
            <td>Allow sink committer and writer operator to be chained together</td>
        </tr>
        <tr>
            <td><h5>sink.cross-partition.managed-memory</h5></td>
            <td style="word-wrap: break-word;">256 mb</td>
            <td>MemorySize</td>
            <td>Weight of managed memory for RocksDB in cross-partition update, Flink will compute the memory size according to the weight, the actual memory used depends on the running environment.</td>
        </tr>
        <tr>
            <td><h5>sink.managed.writer-buffer-memory</h5></td>
            <td style="word-wrap: break-word;">256 mb</td>
            <td>MemorySize</td>
            <td>Weight of writer buffer in managed memory, Flink will compute the memory size for writer according to the weight, the actual memory used depends on the running environment.</td>
        </tr>
        <tr>
            <td><h5>sink.operator-uid.suffix</h5></td>
            <td style="word-wrap: break-word;">(none)</td>
            <td>String</td>
            <td>Set the uid suffix for the writer, dynamic bucket assigner and committer operators. The uid format is ${UID_PREFIX}_${TABLE_NAME}_${USER_UID_SUFFIX}. If the uid suffix is not set, flink will automatically generate the operator uid, which may be incompatible when the topology changes.</td>
        </tr>
        <tr>
            <td><h5>sink.parallelism</h5></td>
            <td style="word-wrap: break-word;">(none)</td>
            <td>Integer</td>
            <td>Defines a custom parallelism for the sink. By default, if this option is not defined, the planner will derive the parallelism for each statement individually by also considering the global configuration.</td>
        </tr>
        <tr>
            <td><h5>sink.savepoint.auto-tag</h5></td>
            <td style="word-wrap: break-word;">false</td>
            <td>Boolean</td>
            <td>If true, a tag will be automatically created for the snapshot created by flink savepoint.</td>
        </tr>
        <tr>
            <td><h5>sink.use-managed-memory-allocator</h5></td>
            <td style="word-wrap: break-word;">false</td>
            <td>Boolean</td>
            <td>If true, flink sink will use managed memory for merge tree; otherwise, it will create an independent memory allocator.</td>
        </tr>
        <tr>
            <td><h5>sink.writer-coordinator.cache-memory</h5></td>
            <td style="word-wrap: break-word;">2 gb</td>
            <td>MemorySize</td>
            <td>Controls the cache memory of writer coordinator to cache manifest files in Job Manager.</td>
        </tr>
        <tr>
            <td><h5>sink.writer-coordinator.enabled</h5></td>
            <td style="word-wrap: break-word;">false</td>
            <td>Boolean</td>
            <td>Enable sink writer coordinator to plan data files in Job Manager.</td>
        </tr>
        <tr>
            <td><h5>sink.writer-coordinator.page-size</h5></td>
            <td style="word-wrap: break-word;">32 kb</td>
            <td>MemorySize</td>
            <td>Controls the page size for one RPC request of writer coordinator.</td>
        </tr>
        <tr>
            <td><h5>sink.writer-cpu</h5></td>
            <td style="word-wrap: break-word;">1.0</td>
            <td>Double</td>
            <td>Sink writer cpu to control cpu cores of writer.</td>
        </tr>
        <tr>
            <td><h5>sink.writer-memory</h5></td>
            <td style="word-wrap: break-word;">(none)</td>
            <td>MemorySize</td>
            <td>Sink writer memory to control heap memory of writer.</td>
        </tr>
        <tr>
            <td><h5>sink.writer-refresh-detectors</h5></td>
            <td style="word-wrap: break-word;">(none)</td>
            <td>String</td>
            <td>The option groups which are expected to be refreshed when streaming writing, multiple option group separated by commas. Now only 'external-paths' is supported.</td>
        </tr>
        <tr>
            <td><h5>source.checkpoint-align.enabled</h5></td>
            <td style="word-wrap: break-word;">false</td>
            <td>Boolean</td>
            <td>Whether to align the flink checkpoint with the snapshot of the paimon table, If true, a checkpoint will only be made if a snapshot is consumed.</td>
        </tr>
        <tr>
            <td><h5>source.checkpoint-align.timeout</h5></td>
            <td style="word-wrap: break-word;">30 s</td>
            <td>Duration</td>
            <td>If the new snapshot has not been generated when the checkpoint starts to trigger, the enumerator will block the checkpoint and wait for the new snapshot. Set the maximum waiting time to avoid infinite waiting, if timeout, the checkpoint will fail. Note that it should be set smaller than the checkpoint timeout.</td>
        </tr>
        <tr>
            <td><h5>source.operator-uid.suffix</h5></td>
            <td style="word-wrap: break-word;">(none)</td>
            <td>String</td>
            <td>Set the uid suffix for the source operators. After setting, the uid format is ${UID_PREFIX}_${TABLE_NAME}_${USER_UID_SUFFIX}. If the uid suffix is not set, flink will automatically generate the operator uid, which may be incompatible when the topology changes.</td>
        </tr>
        <tr>
            <td><h5>unaware-bucket.compaction.parallelism</h5></td>
            <td style="word-wrap: break-word;">(none)</td>
            <td>Integer</td>
            <td>Defines a custom parallelism for the unaware-bucket table compaction job. By default, if this option is not defined, the planner will derive the parallelism for each statement individually by also considering the global configuration.</td>
        </tr>
    </tbody>
</table>
